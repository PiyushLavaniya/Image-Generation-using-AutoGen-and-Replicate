from autogen import AssistantAgent, UserProxyAgent
from autogen import config_list_from_json
from autogen import GroupChat, GroupChatManager
import replicate
import requests
import os
import datetime

os.environ["REPLICATE_API_TOKEN"] = "your Replicate API Key here..."

config_list = config_list_from_json(env_or_file = "OAI_CONFIG_LIST")
llm_config = {"config_list": config_list, "seed": 42, "timeout": 120}


##Creating a function to review the image
def img_review(image_path, prompt):
    response = replicate.run("yorickvp/llava-13b:e272157381e2a3bf12df3a8edd1f38d1dbd736bbb7437277c8b34175f8fce358",
                             input = {
                                 "image": open(image_path, "rb"),
                                 "prompt": f"What is happening inside the image? From scale 1 to 10, decide how similar the image is to the text prompt {prompt}?"
                             }
                            )
    
    results = ""
    for item in response:
        results += item
        
    return results


##Function for Generating the Image

def create_image(prompt):
    results = replicate.run("fofr/latent-consistency-model:683d19dc312f7a9f0428b04429a9ccefd28dbf7785fef083ad5cf991b65f406f",
                            input = {
                                "prompt": prompt
                            }
                            
                        )
    
    if results and len(results) > 0:
        image_url = results[0]
        print(f"{prompt}: {image_url}")
        
        #Downloading the image and saving it based on current time
        current_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        title = prompt[:50]
        
        #Giving the full path to the image where it needs to be saved
        filename = f"images/{title}_{current_time}.png"
        
        response = requests.get(image_url)
        if response.status_code == 200:
            with open(filename, "wb") as f:
                f.write(response.content)
            return f"image saved as {filename}"
        
        else:
            return "Failed to get the image"
        
    else:
        return "Failed to generate the image"
    
        
llm_config_for_assistants =  {
    "functions": [
        {
            "name": "create_image",
            "description": "use latest AI model to generate image based on a prompt, return the file path of image generated",
            "parameters": {
                    "type": "object",
                    "properties": {
                        "prompt": {
                            "type": "string",
                            "description": "a great text to image prompt that describe the image",
                        }
                    },
             "required": ["prompt"],
            },
        },
        
        {
            "name": "img_review",
            "description": "review & critique the AI generated image based on original prompt, decide how can images & prompt can be improved",
            "parameters": {
                    "type": "object",
                    "properties": {
                        "prompt": {
                            "type": "string",
                            "description": "the original prompt used to generate the image",
                        },
                        "image_path": {
                            "type": "string",
                            "description": "the image file path, make sure including the full file path & file extension",
                        }
                    },
                "required": ["prompt", "image_path"],
            },
        },
    ],
    "config_list": config_list,
    "timeout": 120
}

##Let's create our agents that used these functions
##Image generation agent
image_generation_agent = AssistantAgent(
    name = "text_to_img_prompt_expert",
    system_message = "You are a text to image AI model expert, you will use create_image function to generate image with prompt provided, and also improve prompt based on feedback provided until it is 10/10.",
    llm_config = llm_config_for_assistants,
    function_map = {
        "img_review": img_review,
        "create_image": create_image
    }
)

##Critique Agent
image_review_agent = AssistantAgent(
    name ='img_critic',
    system_message = "You are an AI image critique, you will use img_review function to review the image generated by the text_to_img_prompt_expert against the original prompt, and provide feedback on how to improve the prompt.",
    llm_config = llm_config_for_assistants,
    function_map = {
        'img_review': img_review,
        'create_image': create_image
    }  
)

##Let's create our User Proxy agent
user_proxy = UserProxyAgent(
    name = "user_proxy",
    human_input_mode = "ALWAYS",
    function_map = {
        "img_review": img_review,
        "create_image": create_image
    }
)


##Creating the Group Chat
groupchat = GroupChat(
    agents = [user_proxy, image_generation_agent, image_review_agent], messages = [], max_round = 50
)

##Creating our Group Chat Manager, that will Manage the Group Chat
manager = GroupChatManager(
    groupchat = groupchat,
    llm_config = llm_config
)

user_proxy.initiate_chat(
    manager, message = "a photo realistic image of the most beautiful girl sitting on a bench at a park holding an umbrella and smiling at the camera."
)
